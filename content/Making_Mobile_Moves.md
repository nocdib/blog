+++
Categories = ["Bits"]
Description = "Making Mobile Moves"
Tags = ["Android", "New York Times"]
date = "2016-07-26T00:47:59-04:00"
menu = "main"
title = "Making Mobile Moves"

+++

Two months ago I started a new job in the Technology division of <a href="http://www.nytimes.com" target="_blank">The New York Times</a> and I guess I couldn't have started at a better time. Two weeks ago we had Maker's Week, a week-long company <a href="https://en.wikipedia.org/wiki/Hackathon" target="_blank">hackathon</a> where people from the Technology, Design, and Product divisions pitched ideas and worked on projects to be presented at the end of the week. I was part of a three-person group led by a product manager on the Search Team who proposed a mobile app that could take a picture of a Times headline and direct the user to the digital version of the article. The idea was born from her experience commuting to NYC on the <a href="http://www.mta.info/mnr" target="_blank">Metro-North</a> train where she noticed newspaper readers taking pictures of article headlines, presumably to share them or to save them for later. I thought it was a good idea to do a quick proof-of-concept in Android with and we were fortunate enough to have an eager intern with lots of free time and some Android programming experience to join the team.

Our team formed on a Monday and we were set to present on Friday afternoon so time was of the essence for me plus I had other job obligations. The intern was an integral part of the team and really took control of the development while I played a supplemental role. For the technology we used an open source optical character recognition (OCR) library called <a href="https://github.com/rmtheis/tess-two" target="_blank">Tesseract</a> to read the characters from the words in the image and sent that text to an internal search API which would then return the results in order of relevance. Our goal was to identify the most timely article so we just returned the first result or notified the user that no results were returned.

The outcome of our collective effort was successful. Accurate results were returned at a rate of 80% or greater. People were impressed at our presentation and today I had the opportunity to demonstrate the app to higher-ups in the company who are in positions to put the idea in the development pipeline for the flagship Android mobile app. Needless to say I am excited but with the intern soon to return to <a href="http://www.stanford.edu/" target="_blank">Stanford University</a> I will be the only technical point of contact in the office for the project. This means that I have to become very proficient with Android development rather quickly.

Google has <a href="https://www.udacity.com/course/android-development-for-beginners--ud837" target="_blank">some</a> <a href="https://www.udacity.com/course/developing-android-apps--ud853" target="_blank">free</a> <a href="https://www.udacity.com/course/advanced-android-app-development--ud855" target="_blank">videos</a> via Udacity that I found helpful at the beginner level so I'll continue with them for advanced concepts. Once I'm done with those I'll probably begin pursuing the <a href="https://www.udacity.com/course/android-developer-nanodegree-by-google--nd801" target="_blank">Nanodegree in Android Development</a>. For now, I cannot divulge more information about the technical aspects of the project since it may soon become company property but I think I will make a general blog post about OCR recognition on Android using Tesseract in the near future.
